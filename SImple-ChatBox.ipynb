{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7544b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json as js\n",
    "import re\n",
    "\n",
    "with open('E:/Ai project/.ipynb_checkpoints/instents.json', 'r') as f:\n",
    "    data = js.load(f)['intents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b870e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mahan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mahan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca755ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_words(word):\n",
    "    \n",
    "    word = re.sub(\"[^a-z0-9]*\", \"\", word.lower())\n",
    "    word = lemmatizer.lemmatize(word)\n",
    "    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "896a2497",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = []\n",
    "token2idx = {'<PAD>' : 0, '<UNK>' : 1}\n",
    "counter = 2\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for item in data:\n",
    "    label = item['tag'].lower()\n",
    "    \n",
    "    for pattern in item['patterns']:\n",
    "        sentence = ''\n",
    "        \n",
    "        for word in pattern.split():\n",
    "            clean_word = clean_words(word)\n",
    "            sentence += word + ''\n",
    "            \n",
    "            if clean_word not in token2idx.keys():\n",
    "                token2idx[clean_word] = counter\n",
    "                counter += 1\n",
    "                \n",
    "        raw_dataset.append(tuple([sentence.strip(), label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00705197",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(sent) for item in data for sent in item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e40944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sent(sent, max_len, token2isx, PAD = 0, UNK = 1):\n",
    "    \n",
    "    token_list = []\n",
    "    \n",
    "    for token in sent.split():\n",
    "        token = token2idx.get(clean_words(token), UNK)\n",
    "        token_list.append(token)\n",
    "        \n",
    "    if len(token_list) < max_len:\n",
    "        diff = max_len - len(token_list)\n",
    "        token_list.extend([PAD] * diff)\n",
    "    elif len(token_list) > max_len:\n",
    "        token_list = token_list[:max_len]\n",
    "        \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53703b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2label = {item['tag'].lower(): idx for idx, item in enumerate(data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4935e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2tag = {v : k for k, v in tag2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55ab45db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for sentence, tag in raw_dataset:\n",
    "    token_list = tokenize_sent(sentence, max_len, token2idx)\n",
    "    label = tag2label[tag]\n",
    "    dataset.append(tuple([token_list, label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bb7a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = 16, shuffle = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30c84df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classification, self).__init__()\n",
    "        self.embedd = torch.nn.Embedding(len(token2idx.keys()), 16, padding_idx = 0)\n",
    "        self.fc1 = torch.nn.Linear(16, 32)\n",
    "        self.fc2 = torch.nn.Linear(32, len(tag2label.keys()))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedd(x)\n",
    "        x = torch.sum(x, dim = 0)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.log_softmax(self.fc2(x), dim = 1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec03cced",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classification()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd505b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies, losses = [], []\n",
    "\n",
    "total_step = len(dataloader)\n",
    "\n",
    "for epoch in range(100):\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    for i ,(features, labels) in enumerate(dataloader):\n",
    "        fetures = torch.stack(features, dim = 0)\n",
    "        \n",
    "        output = model(fetures)\n",
    "        \n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        correct += (torch.argmax(output, dim = 1) == labels).float().sum()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    accuracy = (100 * correct) / total\n",
    "    accuracies.append(accuracy)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if (epoch +1) % 5 == 0:\n",
    "        print(\n",
    "            f'Epoch [{epoch + 1} / 100], Step[{i + 1} / {total_step}], Loss: {round(loss.item(), 4)}'\n",
    "            f'Accuracy : {accuracy}'\n",
    "        )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cf3902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "while True:\n",
    "    input_sent = input('Enter Your sentence')\n",
    "    token_input = tokenize_sent(input_sent, max_len, token2idx)\n",
    "    input_tensor = torch.tensor(token_input).view(len(token_input), 1)\n",
    "    \n",
    "    output = model(input_tensor)\n",
    "    \n",
    "    label = torch.argmax(output)\n",
    "    tag = label2tag[int(label)]\n",
    "    \n",
    "    for item in data:\n",
    "        if item['tag'] == tag:\n",
    "            break\n",
    "            \n",
    "    print(random.choice(item['responses']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
